{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31a527ac-d497-4f6d-b2f1-09cfcddc99e7",
   "metadata": {},
   "source": [
    "# **BBM409 ASSIGNMENT 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498b1516-1206-44b9-99db-cba49f621931",
   "metadata": {},
   "source": [
    "        Eylül TUNCEL - 21727801\n",
    "        Emre KÖSEN   - 21727498"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d6d5eca-e596-4c53-844c-7f1508f7ad2e",
   "metadata": {},
   "source": [
    "## **Diabetes Risk Prediction**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b190bb4-1b18-4eca-bc4f-aa912543b25c",
   "metadata": {},
   "source": [
    "In this assignment, we implemented a decision tree model to predict whether a patient is a potential diabetic or not. We used ID3 Algorithm on discrete attributes on the dataset and you will apply the discretization process on continuous attribute (”Age” attribute).\n",
    "In the second part of this assignment we prevent overfit our decision tree by pruning the twigs of our tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053ba99-c19b-4f3a-ac52-3ab5262052ca",
   "metadata": {},
   "source": [
    "### Classification Dataset: Diabetes Risk Prediction Dataset\n",
    "\n",
    "• Attribute and Class Information:\n",
    "1. Age (Continuous Attribute)\n",
    "2. Gender (Male, Female)\n",
    "3. Polyuria (Yes, No)\n",
    "4. Polydipsia (Yes, No)\n",
    "5. Sudden Weight Loss (Yes, No)\n",
    "6. Weakness (Yes, No)\n",
    "7. Polyphagia (Yes, No)\n",
    "8. Genital Thrush (Yes, No)\n",
    "9. Visual Blurring (Yes, No)\n",
    "10. Itching (Yes, No)\n",
    "11. Irritability (Yes, No)\n",
    "12. Delayed Healing (Yes, No)\n",
    "13. Partial Paresis (Yes, No)\n",
    "14. Muscle Stiffness (Yes, No)\n",
    "15. Alopecia (Yes, No)\n",
    "16. Obesity (Yes, No)\n",
    "17. Class (Output Prediction Class Information, ”Positive” or ”Negative”)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0e1a7b30-89e1-4989-b0ab-d812ef4d9ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "782f8dcb-1d64-43bb-b052-1fc397a018f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node class for creating decision tree\n",
    "# every node has name -> attribute names\n",
    "#                values -> attribute values [ val1 , val2 , .. ]\n",
    "#                children -> selected child node for sequentially ordered values [ child of val1, child of val2 , ..]\n",
    "#                class_distribution -> positive and negative class distribution of node as [ positive# , negative# ]\n",
    "#                information_gain -> calculated information gain of the node\n",
    "#\n",
    "#        Attribute (Node)       --> name\n",
    "#           /        \\\n",
    "#         yes         no        --> values\n",
    "#         /            \\\n",
    "#       Attr           Attr     --> children\n",
    "class Node(object):\n",
    "    def __init__(self, name, values):\n",
    "        self.name = name\n",
    "        self.values = values\n",
    "        self.information_gain = -1\n",
    "        self.class_distribution = []\n",
    "        self.children = []\n",
    "\n",
    "    def add_child(self, obj):\n",
    "        self.children.append(obj)\n",
    "\n",
    "    def __str__(self, level=0):\n",
    "        ret = \"\\t|\" * level + \"+----\" + repr(self.name) + \"\\n\"\n",
    "        for child in self.children:\n",
    "            ret += child.__str__(level + 1)\n",
    "        return ret\n",
    "\n",
    "    def __repr__(self):\n",
    "        return '<tree node representation>'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7e591a-db72-44da-b09a-1d4a5a02647a",
   "metadata": {},
   "source": [
    "### **Discretization** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df1ef732-d85a-4726-a8e0-0c237ad7f43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# discretization function for \"Age\" attribute which is continuous attribute\n",
    "def discretization(x):\n",
    "    col = []\n",
    "    # for each row of 'Age' column append all values to column array\n",
    "    for i in range(x.shape[0]):\n",
    "        col.append(x[i, 0])\n",
    "    # sort the column array so the first index contains min value, last index contains max value for that column\n",
    "    col.sort()\n",
    "    min_of_col = col[0]\n",
    "    max_of_col = col[x.shape[0] - 1]\n",
    "\n",
    "    # find the interval when we divide the range with 5\n",
    "    interval = (max_of_col - min_of_col + 1) / 5\n",
    "\n",
    "    # for this data set ou age values vary between 16-90\n",
    "    # when we divide this data to 5 groups\n",
    "    #                           group 1 --> between 16 and 30\n",
    "    #                           group 2 --> between 30 and 45\n",
    "    #                           group 3 --> between 45 and 60\n",
    "    #                           group 4 --> between 60 and 75\n",
    "    #                           group 5 --> between 75 and 90\n",
    "    for j in range(x.shape[0]):\n",
    "        # 16->30\n",
    "        if min_of_col <= x[j, 0] < min_of_col + interval:\n",
    "            x[j, 0] = \"group1\"\n",
    "\n",
    "        # 30->45\n",
    "        elif min_of_col + interval <= x[j, 0] < min_of_col + (2 * interval):\n",
    "            x[j, 0] = \"group2\"\n",
    "\n",
    "        # 45->60\n",
    "        elif min_of_col + (2 * interval) <= x[j, 0] < min_of_col + (3 * interval):\n",
    "            x[j, 0] = \"group3\"\n",
    "\n",
    "        # 60->75\n",
    "        elif min_of_col + (3 * interval) <= x[j, 0] < min_of_col + (4 * interval):\n",
    "            x[j, 0] = \"group4\"\n",
    "\n",
    "        # 75->90\n",
    "        elif min_of_col + (4 * interval) <= x[j, 0] < min_of_col + (5 * interval):\n",
    "            x[j, 0] = \"group5\"\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4338f6-17c7-4077-b137-d1fc919847ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold(x):\n",
    "    # start and end points of each fold\n",
    "    size = int(x.shape[0] / 5)\n",
    "    arr = [0, size, 2 * size, 3 * size, 4 * size, 5 * size]\n",
    "    # for each fold, we create our test and train set and then call KNN classification function\n",
    "    for i in range(5):\n",
    "        # 1/5 part of the data set as test data\n",
    "        x_test = x[arr[i]:arr[i + 1]]\n",
    "\n",
    "        # rest of the data set as train data\n",
    "        a = x[0:arr[i]]\n",
    "        b = x[arr[i + 1]:]\n",
    "        x_train = np.concatenate((a, b), axis=0)\n",
    "\n",
    "        attributes = {\"age\": 0, \"gender\": 1, \"polyuria\": 2, \"polydipsia\": 3, \"sudden weight loss\": 4,\n",
    "                      \"weakness\": 5, \"polyphagia\": 6, \"penital thrush\": 7, \"visual blurring\": 8,\n",
    "                      \"itching\": 9, \"irritability\": 10, \"delayed healing\": 11, \"partial paresis\": 12,\n",
    "                      \"muscle stiffness\": 13, \"alopecia\": 14, \"obesity\": 15}\n",
    "\n",
    "        print()\n",
    "        print(\"--------------------------FOLD\", i + 1, \"--------------------------------------------\")\n",
    "\n",
    "        root = ID3(x_train, attributes)\n",
    "        # print(root)\n",
    "\n",
    "        accuracy, precision, recall, f1_score = classification_performance(root, x_test)\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1 Score: \", f1_score)\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6bb4ae7b-70b9-4da5-8e48-a1808d61eafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ID3(data, rem_features):\n",
    "\n",
    "    # find majority of the classes and make a guess\n",
    "    positive, negative = find_most_frequent(data)\n",
    "    if positive > negative:\n",
    "        guess = \"Positive\"\n",
    "    else:\n",
    "        guess = \"Negative\"\n",
    "\n",
    "    # base case: if there arent any positive instances then leaf must be labeled negative\n",
    "    if positive == 0:\n",
    "        return Node(\"LEAF-Negative\", [\"Negative\"])\n",
    "    # base case: if there arent any negative instances then leaf must be labeled positive\n",
    "    elif negative == 0:\n",
    "        return Node(\"LEAF-Positive\", [\"Positive\"])\n",
    "    # base case: if there arent any attributes left to classify, then we must choose majority of the labels as leaf node\n",
    "    elif len(rem_features) == 0:\n",
    "        return Node(\"LEAF-\" + guess, [guess])\n",
    "\n",
    "    else:\n",
    "        # finding distribution of the classes for each of the attributes\n",
    "        class_distribution = find_class_distribution(data, rem_features)\n",
    "\n",
    "        # between all of the remaining attributes, calculate information gain of each of them\n",
    "        info_gain = calculate_info_gain(class_distribution, rem_features)\n",
    "\n",
    "        # select the best node (highest information gain) and make it the next node in the tree\n",
    "        node_name = select_next_node(info_gain)\n",
    "\n",
    "        if node_name == \"\":\n",
    "            return Node(\"LEAF-\" + guess, [guess])\n",
    "\n",
    "        # get all the values of that specific attribute like [yes, no], [male, female]\n",
    "        node_values = []\n",
    "        for x in class_distribution.get(node_name):\n",
    "            if x != \"total\":\n",
    "                node_values.append(x)\n",
    "\n",
    "        features = rem_features.copy()\n",
    "        # pop out selected attribute from remaining features\n",
    "        rem_features.pop(node_name)\n",
    "        node_children = []\n",
    "\n",
    "        for i in node_values:\n",
    "            # take subset of the selected value in the data\n",
    "            subset = np.ndarray([0, 17])\n",
    "            for row in range(len(data)):\n",
    "                val = data[row, features.get(node_name)]\n",
    "                if val.lower() == i:\n",
    "                    subset = np.vstack([subset, data[row, :]])\n",
    "\n",
    "            # to continue making decision tree, call ID3 function recursively\n",
    "            node_children.append(ID3(subset, rem_features.copy()))\n",
    "\n",
    "        # create a node in the tree with selected attributes\n",
    "        node = Node(node_name, node_values)\n",
    "        node.children = node_children\n",
    "        node.information_gain = info_gain.get(node_name)\n",
    "        node.class_distribution = class_distribution.get(node_name).get(\"total\")\n",
    "        return node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dcbca50a-d948-4ac9-8487-04d9d27e36ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding most frequent function is necessary for the base case of the ID3 algorithm\n",
    "# when there is no more attribute left in the tree, we choose most frequent value for the leaf\n",
    "def find_most_frequent(data):\n",
    "    positive_count = 0\n",
    "    negative_count = 0\n",
    "    for i in range(len(data)):\n",
    "        if data[i, 16] == \"Positive\":\n",
    "            positive_count += 1\n",
    "        else:\n",
    "            negative_count += 1\n",
    "    return positive_count, negative_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a9aad420-3cd3-44b0-bff3-c6d70af00cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function returns a class distribution dictionary which is nested dictionary\n",
    "# inside that nested dictionary there is attribute names as value and distribution of attributes as a key, for example;\n",
    "# class_distribution = { 'Gender': {'Male': [18, 36], 'Female': [48, 16]},\n",
    "#                        'Polyuria': {'Yes': '[22, 58], 'No': [18, 20]}     }\n",
    "# the first index in the list shows positive classified sample count and second index shows negatively classified ones\n",
    "def find_class_distribution(x, attributes):\n",
    "    class_distribution = {}\n",
    "\n",
    "    for attr in attributes:\n",
    "        col = attributes.get(attr)\n",
    "\n",
    "        # first column belongs to \"Age\" attribute which has 5 discrete values\n",
    "        if col == 0:\n",
    "            age_dict = {\"group1\": [0, 0],\n",
    "                        \"group2\": [0, 0],\n",
    "                        \"group3\": [0, 0],\n",
    "                        \"group4\": [0, 0],\n",
    "                        \"group5\": [0, 0],\n",
    "                        \"total\": [0, 0]\n",
    "                        }\n",
    "            for row in range(len(x)):\n",
    "                if x[row, col] == \"group1\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        age_dict[\"group1\"][0] += 1\n",
    "                    else:\n",
    "                        age_dict[\"group1\"][1] += 1\n",
    "\n",
    "                elif x[row, col] == \"group2\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        age_dict[\"group2\"][0] += 1\n",
    "                    else:\n",
    "                        age_dict[\"group2\"][1] += 1\n",
    "\n",
    "                elif x[row, col] == \"group3\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        age_dict[\"group3\"][0] += 1\n",
    "                    else:\n",
    "                        age_dict[\"group3\"][1] += 1\n",
    "\n",
    "                elif x[row, col] == \"group4\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        age_dict[\"group4\"][0] += 1\n",
    "                    else:\n",
    "                        age_dict[\"group4\"][1] += 1\n",
    "\n",
    "                elif x[row, col] == \"group5\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        age_dict[\"group5\"][0] += 1\n",
    "                    else:\n",
    "                        age_dict[\"group5\"][1] += 1\n",
    "\n",
    "            # for total value of age attribute distribution\n",
    "            age_dict[\"total\"][0] += age_dict[\"group1\"][0]\n",
    "            age_dict[\"total\"][0] += age_dict[\"group2\"][0]\n",
    "            age_dict[\"total\"][0] += age_dict[\"group3\"][0]\n",
    "            age_dict[\"total\"][0] += age_dict[\"group4\"][0]\n",
    "            age_dict[\"total\"][0] += age_dict[\"group5\"][0]\n",
    "\n",
    "            age_dict[\"total\"][1] += age_dict[\"group1\"][1]\n",
    "            age_dict[\"total\"][1] += age_dict[\"group2\"][1]\n",
    "            age_dict[\"total\"][1] += age_dict[\"group3\"][1]\n",
    "            age_dict[\"total\"][1] += age_dict[\"group4\"][1]\n",
    "            age_dict[\"total\"][1] += age_dict[\"group5\"][1]\n",
    "\n",
    "            # add age attribute  to the class distribution dictionary\n",
    "            class_distribution[\"age\"] = age_dict\n",
    "\n",
    "        # second column is \"Gender\" column which has 2 values (female, male)\n",
    "        elif col == 1:\n",
    "            gender_dict = {\"male\": [0, 0],\n",
    "                           \"female\": [0, 0],\n",
    "                           \"total\": [0, 0]}\n",
    "\n",
    "            # for each row of that specific column\n",
    "            for row in range(len(x)):\n",
    "                if x[row, col] == \"Male\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        gender_dict[\"male\"][0] += 1\n",
    "                    else:\n",
    "                        gender_dict[\"male\"][1] += 1\n",
    "\n",
    "                elif x[row, col] == \"Female\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        gender_dict[\"female\"][0] += 1\n",
    "                    else:\n",
    "                        gender_dict[\"female\"][1] += 1\n",
    "\n",
    "            gender_dict[\"total\"][0] += gender_dict[\"male\"][0]\n",
    "            gender_dict[\"total\"][0] += gender_dict[\"female\"][0]\n",
    "\n",
    "            gender_dict[\"total\"][1] += gender_dict[\"male\"][1]\n",
    "            gender_dict[\"total\"][1] += gender_dict[\"female\"][1]\n",
    "\n",
    "            # add gender attribute  to the class distribution dictionary\n",
    "            class_distribution[\"gender\"] = gender_dict\n",
    "\n",
    "        # all the other attributes has yes/no values\n",
    "        else:\n",
    "            attr_dict = {\"yes\": [0, 0],\n",
    "                         \"no\": [0, 0],\n",
    "                         \"total\": [0, 0]}\n",
    "\n",
    "            # for each row of that specific column\n",
    "            for row in range(len(x)):\n",
    "                if x[row, col] == \"Yes\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        attr_dict[\"yes\"][0] += 1\n",
    "                    else:\n",
    "                        attr_dict[\"yes\"][1] += 1\n",
    "\n",
    "                elif x[row, col] == \"No\":\n",
    "                    if x[row, 16] == \"Positive\":\n",
    "                        attr_dict[\"no\"][0] += 1\n",
    "                    else:\n",
    "                        attr_dict[\"no\"][1] += 1\n",
    "\n",
    "            attr_dict[\"total\"][0] += attr_dict[\"yes\"][0]\n",
    "            attr_dict[\"total\"][0] += attr_dict[\"no\"][0]\n",
    "\n",
    "            attr_dict[\"total\"][1] += attr_dict[\"yes\"][1]\n",
    "            attr_dict[\"total\"][1] += attr_dict[\"no\"][1]\n",
    "\n",
    "            # add that column to the class distribution dictionary\n",
    "            class_distribution[attr] = attr_dict\n",
    "\n",
    "    # print_nested_dict(class_distribution)\n",
    "    return class_distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf316b4-5bbd-4740-9f5d-f187a48bb668",
   "metadata": {},
   "source": [
    "### **Information Gain**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26011b57-89e5-4358-810c-dfe91e550c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating information gain for all remaining attributes given in the parameter\n",
    "def calculate_info_gain(dist, attributes):\n",
    "    # dictionary for attributes and respective information gains of them\n",
    "    info_gain = {\"age\": 0, \"gender\": 0, \"polyuria\": 0, \"polydipsia\": 0, \"sudden weight loss\": 0,\n",
    "                 \"weakness\": 0, \"polyphagia\": 0, \"penital thrush\": 0, \"visual blurring\": 0,\n",
    "                 \"itching\": 0, \"irritability\": 0, \"delayed healing\": 0, \"partial paresis\": 0,\n",
    "                 \"muscle stiffness\": 0, \"alopecia\": 0, \"obesity\": 0}\n",
    "    for attr in attributes:\n",
    "        gain = calculate_entropy(dist.get(attr).get(\"total\"))\n",
    "        for i in dist.get(attr):\n",
    "            if i != \"total\":\n",
    "                sv = dist.get(attr).get(i)[0] + dist.get(attr).get(i)[1]\n",
    "                s = dist.get(attr).get(\"total\")[0] + dist.get(attr).get(\"total\")[1]\n",
    "                ent_sv = calculate_entropy(dist.get(attr).get(i))\n",
    "                gain -= abs(sv / s) * ent_sv\n",
    "        info_gain[attr] = gain\n",
    "    return info_gain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6d02844-f2e5-40c4-be8c-0fa69957c1ef",
   "metadata": {},
   "source": [
    "### **Entrophy**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8cde0ddc-1054-4f1f-a96d-cc591d1fa9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this function calculates entropy of the given positive and negative values\n",
    "# parameter \"dist\" is distribution of positive and negative values for an attribute\n",
    "def calculate_entropy(dist):\n",
    "    total = dist[0] + dist[1]\n",
    "    if total == 0:\n",
    "        return 1\n",
    "    pos_prop = dist[0] / total\n",
    "    neg_prop = dist[1] / total\n",
    "    log = lambda prop: math.log(prop, 2) if prop != 0 else 0\n",
    "    entropy = -(pos_prop * log(pos_prop)) - (neg_prop * log(neg_prop))\n",
    "    return entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2725a5c0-5ba7-478b-b26e-33e5515a855c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# next node of the tree is selected based on this function,\n",
    "# we calculate information gain for all remaining attributes, and select best of it for the next node\n",
    "def select_next_node(info_gain):\n",
    "    max_info_gain = 0\n",
    "    best_attr = \"\"\n",
    "    for x in info_gain:\n",
    "        if info_gain.get(x) > max_info_gain:\n",
    "            best_attr = x\n",
    "            max_info_gain = info_gain.get(x)\n",
    "    return best_attr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866c1cce-ff0f-483e-be43-d84217d14297",
   "metadata": {},
   "source": [
    "### **Performance Metrics** \n",
    "#### Accuracy\n",
    "#### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "50029dcc-1854-4c1b-8f6f-b38aa389cf3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for testing performance we use accuracy, recall, precision and f1 score metrics\n",
    "# those metrics are calculated in the function below\n",
    "def classification_performance(root, x_test):\n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    for sample in x_test:\n",
    "        prediction = decision_tree_test(root, sample)\n",
    "        if prediction == sample[16] and prediction == \"Positive\":\n",
    "            tp += 1\n",
    "        elif prediction == sample[16] and prediction == \"Negative\":\n",
    "            tn += 1\n",
    "        if prediction != sample[16] and prediction == \"Positive\":\n",
    "            fp += 1\n",
    "        elif prediction != sample[16] and prediction == \"Negative\":\n",
    "            fn += 1\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1_score = (2 * recall * precision) / (recall + precision)\n",
    "    return accuracy, precision, recall, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bb9f373-66d8-4f0f-a358-f9b256221de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_test(node, test):\n",
    "    # attributes and their respective indices in the data is given with dictionary\n",
    "    attributes = {\"age\": 0, \"gender\": 1, \"polyuria\": 2, \"polydipsia\": 3, \"sudden weight loss\": 4,\n",
    "                  \"weakness\": 5, \"polyphagia\": 6, \"penital thrush\": 7, \"visual blurring\": 8,\n",
    "                  \"itching\": 9, \"irritability\": 10, \"delayed healing\": 11, \"partial paresis\": 12,\n",
    "                  \"muscle stiffness\": 13, \"alopecia\": 14, \"obesity\": 15}\n",
    "\n",
    "    # if it is leaf node then return the class of if (Positive/Negative)\n",
    "    if node.name == \"LEAF-Positive\" or node.name == \"LEAF-Negative\":\n",
    "        return node.values[0]\n",
    "    # if it ii one of the internal nodes, then go one more level down in decision tree\n",
    "    else:\n",
    "        for val in node.values:\n",
    "            if test[attributes.get(node.name)].lower() == val:\n",
    "                index = node.values.index(val)\n",
    "                child_node = node.children[index]\n",
    "                return decision_tree_test(child_node, test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c6207ff-9b86-4252-862b-b6597fd5acd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_fold_and_prune(x):\n",
    "    # start and end points of each fold\n",
    "    size = int(x.shape[0] / 5)\n",
    "    arr = [0, size, 2 * size, 3 * size, 4 * size, 5 * size]\n",
    "    # for each fold, we create our test and train set and then call KNN classification function\n",
    "    for i in range(5):\n",
    "        # 1/5 part of the data set as validation set\n",
    "        x_validation = x[arr[i]:arr[i + 1]]\n",
    "\n",
    "        if i != 4:\n",
    "            # 1/5 part of the data set as test set\n",
    "            x_test = x[arr[i + 1]:arr[i + 2]]\n",
    "            # rest of the data set as train data\n",
    "            a = x[0:arr[i]]\n",
    "            b = x[arr[i + 2]:]\n",
    "            x_train = np.concatenate((a, b), axis=0)\n",
    "        else:\n",
    "            x_test = x[0:arr[1]]\n",
    "            x_train = x[arr[1]:arr[4]]\n",
    "\n",
    "        attributes = {\"age\": 0, \"gender\": 1, \"polyuria\": 2, \"polydipsia\": 3, \"sudden weight loss\": 4,\n",
    "                      \"weakness\": 5, \"polyphagia\": 6, \"penital thrush\": 7, \"visual blurring\": 8,\n",
    "                      \"itching\": 9, \"irritability\": 10, \"delayed healing\": 11, \"partial paresis\": 12,\n",
    "                      \"muscle stiffness\": 13, \"alopecia\": 14, \"obesity\": 15}\n",
    "\n",
    "        print()\n",
    "        print(\"--------------------------FOLD\", i + 1, \"--------------------------------------------\")\n",
    "\n",
    "        root = ID3(x_train, attributes)\n",
    "        # print(root)\n",
    "\n",
    "        accuracy = classification_performance(root, x_test)[0]\n",
    "        print(\"Test Accuracy Before Pruning :\", accuracy)\n",
    "        print()\n",
    "\n",
    "        root = prune_tree(root, x_validation)\n",
    "        # print(root)\n",
    "\n",
    "        accuracy, precision, recall, f1_score = classification_performance(root, x_test)\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        print(\"Precision: \", precision)\n",
    "        print(\"Recall: \", recall)\n",
    "        print(\"F1 Score: \", f1_score)\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ccb5bf6-fe27-4d22-9139-5564807f60c6",
   "metadata": {},
   "source": [
    "### **Decision Tree Pruning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c40d5a4c-d3f8-4ca2-b986-f64ae1b06907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prune tree function identifies the twigs which are not necessary for prediction,  based on the information gain\n",
    "def prune_tree(root, x_validation):\n",
    "    current_accuracy = classification_performance(root, x_validation)[0]\n",
    "    last_accuracy = classification_performance(root, x_validation)[0]\n",
    "\n",
    "    # finding least informative twig based on the information gain values\n",
    "    twig = find_least_informative_twig(root)\n",
    "\n",
    "    # while accuracy is not changing or increasing, we keep pruning the tree\n",
    "    while current_accuracy >= last_accuracy:\n",
    "        # save those information for reverting the last changes if necessary\n",
    "        name = twig.name\n",
    "        values = twig.values\n",
    "        children = twig.children\n",
    "        info_gain = twig.information_gain\n",
    "\n",
    "        # based on majority of class(positive/negative) we replace twig with a leaf\n",
    "        if twig.class_distribution[0] >= twig.class_distribution[1]:\n",
    "            twig.name = \"LEAF-Positive\"\n",
    "            twig.values = [\"Positive\"]\n",
    "            twig.children = []\n",
    "            twig.information_gain = -1\n",
    "        elif twig.class_distribution[1] > twig.class_distribution[0]:\n",
    "            twig.name = \"LEAF-Negative\"\n",
    "            twig.values = [\"Negative\"]\n",
    "            twig.children = []\n",
    "            twig.information_gain = -1\n",
    "\n",
    "        last_accuracy = current_accuracy\n",
    "        current_accuracy = classification_performance(root, x_validation)[0]\n",
    "\n",
    "        # if current accuracy decreases then we must revert the changes we made in the last twig\n",
    "        if current_accuracy < last_accuracy:\n",
    "            twig.name = name\n",
    "            twig.values = values\n",
    "            twig.children = children\n",
    "            twig.information_gain = info_gain\n",
    "            current_accuracy = classification_performance(root, x_validation)[0]\n",
    "            break\n",
    "\n",
    "        twig = find_least_informative_twig(root)\n",
    "    return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7867209c-d105-4755-a647-9ef8f7f89188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding least informative twig requires to look all of the twigs and their respective information gain values\n",
    "# for traversing the tree we use preorder traversal method and find all twigs\n",
    "def find_least_informative_twig(root):\n",
    "    all_twigs = []\n",
    "    # all twigs are collected at the end of his function\n",
    "    preorder_traversal_util(root, all_twigs)\n",
    "    info_gain = 1\n",
    "    least_informative_twig = None\n",
    "    # between all twigs search for the least informative one\n",
    "    for twig in all_twigs:\n",
    "        if info_gain >= twig.information_gain:\n",
    "            info_gain = twig.information_gain\n",
    "            least_informative_twig = twig\n",
    "    return least_informative_twig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a718f842-9c96-4383-a91c-ffd986b4be92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in preorder traversal of tree we search all nodes to find twigs in the tree\n",
    "# twigs are the nodes which are their all children were leaves\n",
    "def preorder_traversal_util(root, all_twigs):\n",
    "    # when found a leaf, return ( base case)\n",
    "    if root.values[0] == \"Positive\" or root.values[0] == \"Negative\":\n",
    "        return\n",
    "\n",
    "    # if the node is not leaf then look if its all children are leaves ( twig node )\n",
    "    leaf_count = 0\n",
    "    for item in root.children:\n",
    "        if item.values[0] == \"Positive\" or item.values[0] == \"Negative\":\n",
    "            leaf_count += 1\n",
    "    if leaf_count == len(root.children):\n",
    "        all_twigs.append(root)\n",
    "\n",
    "    # recursively call the function to continue searching\n",
    "    for child in root.children:\n",
    "        preorder_traversal_util(child, all_twigs)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "49f1ec7b-88bc-4ded-a94e-76356e537309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data's in the csv file to the numpy array\n",
    "df = pd.read_csv('./diabetes_data_upload.csv')\n",
    "x = np.array(df.iloc[:, :])\n",
    "\n",
    "# discretization on age attribute\n",
    "x = discretization(x)\n",
    "\n",
    "# shuffle the data\n",
    "np.random.seed(101)\n",
    "np.random.shuffle(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "16bd7ec4-8e35-4ebb-ae34-3cb587a1d296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------FOLD 1 --------------------------------------------\n",
      "Accuracy:  0.9134615384615384\n",
      "Precision:  0.9193548387096774\n",
      "Recall:  0.9344262295081968\n",
      "F1 Score:  0.9268292682926829\n",
      "\n",
      "--------------------------FOLD 2 --------------------------------------------\n",
      "Accuracy:  0.9807692307692307\n",
      "Precision:  0.984375\n",
      "Recall:  0.984375\n",
      "F1 Score:  0.984375\n",
      "\n",
      "--------------------------FOLD 3 --------------------------------------------\n",
      "Accuracy:  0.9519230769230769\n",
      "Precision:  0.9508196721311475\n",
      "Recall:  0.9666666666666667\n",
      "F1 Score:  0.9586776859504132\n",
      "\n",
      "--------------------------FOLD 4 --------------------------------------------\n",
      "Accuracy:  0.9230769230769231\n",
      "Precision:  0.9710144927536232\n",
      "Recall:  0.9178082191780822\n",
      "F1 Score:  0.943661971830986\n",
      "\n",
      "--------------------------FOLD 5 --------------------------------------------\n",
      "Accuracy:  0.9711538461538461\n",
      "Precision:  0.9836065573770492\n",
      "Recall:  0.967741935483871\n",
      "F1 Score:  0.975609756097561\n"
     ]
    }
   ],
   "source": [
    "k_fold(x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9bd3d3f4-af7c-4aae-98d0-e4afa3e14575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------FOLD 1 --------------------------------------------\n",
      "Test Accuracy Before Pruning : 0.9230769230769231\n",
      "\n",
      "Accuracy:  0.9326923076923077\n",
      "Precision:  0.9672131147540983\n",
      "Recall:  0.921875\n",
      "F1 Score:  0.944\n",
      "\n",
      "--------------------------FOLD 2 --------------------------------------------\n",
      "Test Accuracy Before Pruning : 0.9423076923076923\n",
      "\n",
      "Accuracy:  0.9519230769230769\n",
      "Precision:  0.9508196721311475\n",
      "Recall:  0.9666666666666667\n",
      "F1 Score:  0.9586776859504132\n",
      "\n",
      "--------------------------FOLD 3 --------------------------------------------\n",
      "Test Accuracy Before Pruning : 0.8942307692307693\n",
      "\n",
      "Accuracy:  0.9134615384615384\n",
      "Precision:  0.9571428571428572\n",
      "Recall:  0.9178082191780822\n",
      "F1 Score:  0.9370629370629371\n",
      "\n",
      "--------------------------FOLD 4 --------------------------------------------\n",
      "Test Accuracy Before Pruning : 0.9711538461538461\n",
      "\n",
      "Accuracy:  0.9807692307692307\n",
      "Precision:  0.9838709677419355\n",
      "Recall:  0.9838709677419355\n",
      "F1 Score:  0.9838709677419355\n",
      "\n",
      "--------------------------FOLD 5 --------------------------------------------\n",
      "Test Accuracy Before Pruning : 0.9423076923076923\n",
      "\n",
      "Accuracy:  0.9423076923076923\n",
      "Precision:  0.9230769230769231\n",
      "Recall:  0.9836065573770492\n",
      "F1 Score:  0.9523809523809524\n"
     ]
    }
   ],
   "source": [
    "k_fold_and_prune(x.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d98949-ae94-48e0-8cf5-f55d7d94da2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fa5fee84-9164-47b8-bdea-4d2b7deb50a0",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d935437d-b85c-4515-9816-a3d1f691692e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3c2d2cb-2826-4a5e-be83-1ee47df22ba2",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
